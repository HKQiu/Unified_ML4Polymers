{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已计算 20 个描述符\n",
      "已计算 40 个描述符\n",
      "已计算 60 个描述符\n",
      "已计算 80 个描述符\n",
      "已计算 100 个描述符\n",
      "已计算 120 个描述符\n",
      "已计算 140 个描述符\n",
      "已计算 160 个描述符\n",
      "已计算 180 个描述符\n",
      "已计算 200 个描述符\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit import Chem\n",
    "df = pd.read_csv('/home/hkqiu/work/PolyGPT/polymer properties/reg-cls/exp_val/8个验证Tg的smiles.csv')\n",
    "can_smiles = df['Smiles'].tolist()\n",
    "\n",
    "# 计算描述符并添加到DataFrame中\n",
    "num_processed = 0\n",
    "print_frequency = 20\n",
    "\n",
    "for desc_name, desc_func in Descriptors.descList:\n",
    "    try:\n",
    "        descriptors = [desc_func(Chem.MolFromSmiles(smiles)) if Chem.MolFromSmiles(smiles) else None for smiles in can_smiles] \n",
    "    except Exception as e:\n",
    "        print(f\"SMILES Parse Error: {e}\")\n",
    "        descriptors = None\n",
    "    df[desc_name] = descriptors\n",
    "\n",
    "    # 打印处理进度\n",
    "    num_processed += 1\n",
    "    if num_processed % print_frequency == 0:\n",
    "        print(f\"已计算 {num_processed} 个描述符\")\n",
    "\n",
    "# 保存包含描述符的DataFrame到CSV文件\n",
    "df.to_csv('/home/hkqiu/work/PolyGPT/polymer properties/reg-cls/exp_val/date_with_descriptors.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.svm import NuSVR\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error,r2_score, mean_absolute_error\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['atomization energy', 'bandgap-crystal', 'Tg',\n",
       "       'heat resistance class'], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('/home/hkqiu/work/PolyGPT/T5/0data&code/data/train/merge/merged_date_with_descriptors_filtered_data.csv')\n",
    "test_df = pd.read_csv('/home/hkqiu/work/PolyGPT/T5/0data&code/data/test/merge/merged_date_with_descriptors_filtered_data.csv')\n",
    "\n",
    "\n",
    "X_train = train_df.drop(['prompt','target'], axis=1)\n",
    "X_test = test_df.drop(['prompt','target'], axis=1)\n",
    "y_train = train_df[['task','target']]\n",
    "y_test = test_df[['task','target']]\n",
    "\n",
    "\n",
    "unique_tasks = X_train['task'].unique()\n",
    "unique_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set:\n",
      "atomization energy: (0, 5264)\n",
      "bandgap-crystal: (5265, 9504)\n",
      "Tg: (9505, 15654)\n",
      "heat resistance class: (15655, 20649)\n",
      "Test set:\n",
      "atomization energy: (0, 584)\n",
      "bandgap-crystal: (585, 1064)\n",
      "Tg: (1065, 1764)\n",
      "heat resistance class: (1765, 2319)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# 假设元素类别列表\n",
    "categories = ['atomization energy', 'bandgap-crystal', 'Tg',\n",
    "       'heat resistance class']\n",
    "\n",
    "# 创建一个空字典来存储每种元素对应的索引范围\n",
    "train_index_ranges = {}\n",
    "test_index_ranges = {}\n",
    "\n",
    "# 遍历每个元素类别\n",
    "for category in categories:\n",
    "    # 获取特定元素类别的索引范围\n",
    "    indices = train_df[train_df['task'] == category].index\n",
    "    train_index_ranges[category] = (indices.min(), indices.max())\n",
    "\n",
    "# 打印每种元素对应的索引范围\n",
    "print(\"Train set:\")\n",
    "for category, index_range in train_index_ranges.items():\n",
    "    \n",
    "    print(f\"{category}: {index_range}\")\n",
    "\n",
    "\n",
    "# 遍历每个元素类别\n",
    "for category in categories:\n",
    "    # 获取特定元素类别的索引范围\n",
    "    indices = test_df[test_df['task'] == category].index\n",
    "    test_index_ranges[category] = (indices.min(), indices.max())\n",
    "\n",
    "# 打印每种元素对应的索引范围\n",
    "print(\"Test set:\")\n",
    "for category, index_range in test_index_ranges.items():\n",
    "    \n",
    "    print(f\"{category}: {index_range}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(property):\n",
    "    \"\"\"\n",
    "    a:tg_X_train\n",
    "    b:tg_X_test\n",
    "    c:tg_y_train\n",
    "    d:tg_y_test\n",
    "    \"\"\"\n",
    "    a = X_train[X_train['task']==property].drop('task', axis=1)\n",
    "    b = X_test[X_test['task']==property].drop('task', axis=1)\n",
    "    c = y_train[y_train['task']==property].drop('task', axis=1)\n",
    "    d = y_test[y_test['task']==property].drop('task', axis=1)\n",
    "\n",
    "    return a,b,c,d\n",
    "\n",
    "\"\"\"['band gap chain', 'atomization energy', 'ionization energy',\n",
    "       'band gap bulk', 'electron affinity', 'bandgap-crystal', 'Tg',\n",
    "       'crystallization tendency', 'dielectric constant',\n",
    "       'refractive index', 'heat resistance class']\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "tg_X_train, tg_X_test, tg_y_train, tg_y_test = get_data('Tg')\n",
    "bc_X_train, bc_X_test, bc_y_train, bc_y_test = get_data('bandgap-crystal')\n",
    "ae_X_train, ae_X_test, ae_y_train, ae_y_test = get_data('atomization energy')\n",
    "\n",
    "hrc_X_train, hrc_X_test, hrc_y_train, hrc_y_test = get_data('heat resistance class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty DataFrame to store the model performance data\n",
    "performance_data = pd.DataFrame(columns=['Model', 'R2', 'MAE', 'MSE'])\n",
    "\n",
    "def save_performance_data(model_name, r2, mae, mse):\n",
    "    global performance_data\n",
    "    new_row = pd.DataFrame([[model_name, r2, mae, mse]], columns=['Model', 'R2', 'MAE', 'MSE'])\n",
    "    performance_data = pd.concat([performance_data, new_row], ignore_index=True)\n",
    "\n",
    "def RandomForest(X_train, X_test, y_train, y_test):\n",
    "    model = RandomForestRegressor(n_estimators=1, random_state=42)\n",
    "\n",
    "    # 在训练集上训练模型\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # 在测试集上进行预测\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # 计算准确率\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    MSE = mean_squared_error(y_test, y_pred)\n",
    "    save_performance_data('RandomForest', r2, mae, MSE)\n",
    "    print(f\"RandomForest R2: {r2}\")\n",
    "    print(f\"RandomForest MAE: {mae}\")\n",
    "    print(f\"RandomForest MSE: {MSE}\")\n",
    "    print(\"=======================\")\n",
    "\n",
    "def Linear(X_train, X_test, y_train, y_test):\n",
    "    model = LinearRegression()\n",
    "\n",
    "    # 在训练集上训练模型\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # 在测试集上进行预测\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # 计算准确率\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    MSE = mean_squared_error(y_test, y_pred)\n",
    "    save_performance_data('Linear', r2, mae, MSE)\n",
    "    print(f\"LinearRegression R2: {r2}\")\n",
    "    print(f\"LinearRegression MAE: {mae}\")\n",
    "    print(f\"LinearRegression MSE: {MSE}\")\n",
    "    print(\"=======================\")\n",
    "\n",
    "def Svr(X_train, X_test, y_train, y_test):\n",
    "    model = SVR()\n",
    "\n",
    "    # 在训练集上训练模型\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # 在测试集上进行预测\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # 计算准确率\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    MSE = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    print(f\"SVR R2: {r2}\")\n",
    "    print(f\"SVR MAE: {mae}\")\n",
    "    print(f\"SVR MSE: {MSE}\")\n",
    "    print(\"=======================\")\n",
    "\n",
    "    # 保存性能数据\n",
    "    save_performance_data(\"SVR\", r2, mae, MSE)\n",
    "\n",
    "def DecisionTree(X_train, X_test, y_train, y_test):\n",
    "    model = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "    # 在训练集上训练模型\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # 在测试集上进行预测\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # 计算准确率\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    MSE = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    print(f\"DecisionTree R2: {r2}\")\n",
    "    print(f\"DecisionTree MAE: {mae}\")\n",
    "    print(f\"DecisionTree MSE: {MSE}\")\n",
    "    print(\"=======================\")\n",
    "\n",
    "    # 保存性能数据\n",
    "    save_performance_data(\"DecisionTree\", r2, mae, MSE)\n",
    "\n",
    "def RidgeRegression(X_train, X_test, y_train, y_test):\n",
    "    model = Ridge(alpha=1.0)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    MSE = mean_squared_error(y_test, y_pred)\n",
    "    print(f\"Ridge Regression R2: {r2}\")\n",
    "    print(f\"Ridge Regression MAE: {mae}\")\n",
    "    print(f\"Ridge Regression MSE: {MSE}\")\n",
    "    print(\"=======================\")\n",
    "\n",
    "    # 保存性能数据\n",
    "    save_performance_data(\"Ridge Regression\", r2, mae, MSE)\n",
    "\n",
    "def GaussianProcess(X_train, X_test, y_train, y_test):\n",
    "    model = GaussianProcessRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    MSE = mean_squared_error(y_test, y_pred)\n",
    "    print(f\"GaussianProcessRegressor R2: {r2}\")\n",
    "    print(f\"GaussianProcessRegressor MAE: {mae}\")\n",
    "    print(f\"GaussianProcessRegressor MSE: {MSE}\")\n",
    "    print(\"=======================\")\n",
    "\n",
    "    # 保存性能数据\n",
    "    save_performance_data(\"GaussianProcessRegressor\", r2, mae, MSE)\n",
    "\n",
    "def AdaBoost(X_train, X_test, y_train, y_test):\n",
    "    model = AdaBoostRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    MSE = mean_squared_error(y_test, y_pred)\n",
    "    save_performance_data(\"AdaBoost\", r2, mae, MSE)\n",
    "    print(f\"AdaBoostRegressor R2: {r2}\")\n",
    "    print(f\"AdaBoostRegressor MAE: {mae}\")\n",
    "    print(f\"AdaBoostRegressor MSE: {MSE}\")\n",
    "    print(\"=======================\")\n",
    "\n",
    "def GradientBoosting(X_train, X_test, y_train, y_test):\n",
    "    model = GradientBoostingRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    MSE = mean_squared_error(y_test, y_pred)\n",
    "    save_performance_data(\"GradientBoosting\", r2, mae, MSE)\n",
    "    print(f\"GradientBoostingRegressor R2: {r2}\")\n",
    "    print(f\"GradientBoostingRegressor MAE: {mae}\")\n",
    "    print(f\"GradientBoostingRegressor MSE: {MSE}\")\n",
    "    print(\"=======================\")\n",
    "\n",
    "def Bagging(X_train, X_test, y_train, y_test):\n",
    "    model = BaggingRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    MSE = mean_squared_error(y_test, y_pred)\n",
    "    save_performance_data(\"Bagging\", r2, mae, MSE)\n",
    "    print(f\"BaggingRegressor R2: {r2}\")\n",
    "    print(f\"BaggingRegressor MAE: {mae}\")\n",
    "    print(f\"BaggingRegressor MSE: {MSE}\")\n",
    "    print(\"=======================\")\n",
    "\n",
    "def ExtraTrees(X_train, X_test, y_train, y_test):\n",
    "    model = ExtraTreesRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    MSE = mean_squared_error(y_test, y_pred)\n",
    "    save_performance_data(\"ExtraTrees\", r2, mae, MSE)\n",
    "    print(f\"ExtraTreesRegressor R2: {r2}\")\n",
    "    print(f\"ExtraTreesRegressor MAE: {mae}\")\n",
    "    print(f\"ExtraTreesRegressor MSE: {MSE}\")\n",
    "    print(\"=======================\")\n",
    "\n",
    "\n",
    "def train_predict(X_train, X_test, y_train, y_test):\n",
    "    RandomForest(X_train, X_test, y_train, y_test)\n",
    "    Linear(X_train, X_test, y_train, y_test)\n",
    "    Svr(X_train, X_test, y_train, y_test)\n",
    "    DecisionTree(X_train, X_test, y_train, y_test)\n",
    "    RidgeRegression(X_train, X_test, y_train, y_test)\n",
    "    GaussianProcess(X_train, X_test, y_train, y_test)\n",
    "    AdaBoost(X_train, X_test, y_train, y_test)\n",
    "    Bagging(X_train, X_test, y_train, y_test)\n",
    "    ExtraTrees(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Tg***\n",
      "RandomForest R2: 0.7877011170752712\n",
      "RandomForest MAE: 40.48248727880983\n",
      "RandomForest MSE: 3671.944417743777\n",
      "=======================\n",
      "LinearRegression R2: -0.00017849022655913593\n",
      "LinearRegression MAE: 111.20036190435586\n",
      "LinearRegression MSE: 17299.19523522385\n",
      "=======================\n",
      "SVR R2: -0.06575069272608647\n",
      "SVR MAE: 106.07285704606691\n",
      "SVR MSE: 18433.339134665242\n",
      "=======================\n",
      "DecisionTree R2: 0.7871190673834627\n",
      "DecisionTree MAE: 39.0525\n",
      "DecisionTree MSE: 3682.0116121031742\n",
      "=======================\n",
      "Ridge Regression R2: 0.8910406051493795\n",
      "Ridge Regression MAE: 32.17445792077355\n",
      "Ridge Regression MSE: 1884.5734662877614\n",
      "=======================\n",
      "GaussianProcessRegressor R2: -1.905496239946293\n",
      "GaussianProcessRegressor MAE: 188.0253322547167\n",
      "GaussianProcessRegressor MSE: 50253.77690201503\n",
      "=======================\n",
      "AdaBoostRegressor R2: 0.8683799411630398\n",
      "AdaBoostRegressor MAE: 35.934305296038396\n",
      "AdaBoostRegressor MSE: 2276.5147590571137\n",
      "=======================\n",
      "BaggingRegressor R2: 0.8359317519911869\n",
      "BaggingRegressor MAE: 30.63707833220199\n",
      "BaggingRegressor MSE: 2837.7421449672133\n",
      "=======================\n",
      "ExtraTreesRegressor R2: 0.8477586146264753\n",
      "ExtraTreesRegressor MAE: 30.590861904761905\n",
      "ExtraTreesRegressor MSE: 2633.1834509468254\n",
      "=======================\n"
     ]
    }
   ],
   "source": [
    "print(\"***Tg***\")\n",
    "train_predict(tg_X_train, tg_X_test, tg_y_train, tg_y_test)\n",
    "performance_data.to_csv('/home/hkqiu/work/PolyGPT/T5/0data&code/ml models/model_performance/Tg.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***bandgap_crystal***\n",
      "RandomForest R2: 0.7726795069833294\n",
      "RandomForest MAE: 0.803925\n",
      "RandomForest MSE: 1.003354361666667\n",
      "=======================\n",
      "LinearRegression R2: 0.583020059358073\n",
      "LinearRegression MAE: 0.9918139570307278\n",
      "LinearRegression MSE: 1.8404792133716836\n",
      "=======================\n",
      "SVR R2: -0.10742247588789988\n",
      "SVR MAE: 1.7029543823655093\n",
      "SVR MSE: 4.887976251698249\n",
      "=======================\n",
      "DecisionTree R2: 0.7602583212723335\n",
      "DecisionTree MAE: 0.8172874999999997\n",
      "DecisionTree MSE: 1.0581793829166666\n",
      "=======================\n",
      "Ridge Regression R2: 0.8242596072821184\n",
      "Ridge Regression MAE: 0.6467277824526799\n",
      "Ridge Regression MSE: 0.7756884881538952\n",
      "=======================\n",
      "GaussianProcessRegressor R2: -5.984309079857041\n",
      "GaussianProcessRegressor MAE: 5.139429166666667\n",
      "GaussianProcessRegressor MSE: 30.827563698749994\n",
      "=======================\n",
      "AdaBoostRegressor R2: 0.8308044513458083\n",
      "AdaBoostRegressor MAE: 0.6708269388039324\n",
      "AdaBoostRegressor MSE: 0.7468006490040402\n",
      "=======================\n",
      "BaggingRegressor R2: 0.8263578736730555\n",
      "BaggingRegressor MAE: 0.6479037499999999\n",
      "BaggingRegressor MSE: 0.7664270937791665\n",
      "=======================\n",
      "ExtraTreesRegressor R2: 0.8568143755853017\n",
      "ExtraTreesRegressor MAE: 0.6329189583333339\n",
      "ExtraTreesRegressor MSE: 0.6319972250540424\n",
      "=======================\n"
     ]
    }
   ],
   "source": [
    "print(\"***bandgap_crystal***\")\n",
    "performance_data = pd.DataFrame(columns=['Model', 'R2', 'MAE', 'MSE'])\n",
    "train_predict(bc_X_train, bc_X_test, bc_y_train, bc_y_test)\n",
    "performance_data.to_csv('./model_performance/bandgap_crystal.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***atomization energy***\n",
      "RandomForest R2: 0.955150215095907\n",
      "RandomForest MAE: 0.0497435897435901\n",
      "RandomForest MSE: 0.005025641025641048\n",
      "=======================\n",
      "LinearRegression R2: -14.791845637509647\n",
      "LinearRegression MAE: 0.2795157935647559\n",
      "LinearRegression MSE: 1.7695546918713607\n",
      "=======================\n",
      "SVR R2: -0.010928641842724574\n",
      "SVR MAE: 0.27042470423729176\n",
      "SVR MSE: 0.11327957240608147\n",
      "=======================\n",
      "DecisionTree R2: 0.917531313880434\n",
      "DecisionTree MAE: 0.06871794871794865\n",
      "DecisionTree MSE: 0.00924102564102558\n",
      "=======================\n",
      "Ridge Regression R2: 0.9552727203657102\n",
      "Ridge Regression MAE: 0.05417531247403009\n",
      "Ridge Regression MSE: 0.005011913702063093\n",
      "=======================\n",
      "GaussianProcessRegressor R2: -318.3582902954816\n",
      "GaussianProcessRegressor MAE: 5.96007647623907\n",
      "GaussianProcessRegressor MSE: 35.78568167093004\n",
      "=======================\n",
      "AdaBoostRegressor R2: 0.8797193381717081\n",
      "AdaBoostRegressor MAE: 0.0926267400743825\n",
      "AdaBoostRegressor MSE: 0.01347804521176991\n",
      "=======================\n",
      "BaggingRegressor R2: 0.9650000058673188\n",
      "BaggingRegressor MAE: 0.04028205128205136\n",
      "BaggingRegressor MSE: 0.003921923076923073\n",
      "=======================\n",
      "ExtraTreesRegressor R2: 0.9682058410332102\n",
      "ExtraTreesRegressor MAE: 0.036512820512821134\n",
      "ExtraTreesRegressor MSE: 0.0035626933333334754\n",
      "=======================\n"
     ]
    }
   ],
   "source": [
    "print(\"***atomization energy***\")\n",
    "performance_data = pd.DataFrame(columns=['Model', 'R2', 'MAE', 'MSE'])\n",
    "train_predict(ae_X_train, ae_X_test, ae_y_train, ae_y_test)\n",
    "performance_data.to_csv('./model_performance/atomization energy.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomForest(X_train, X_test, y_train, y_test):\n",
    "    model = RandomForestRegressor(n_estimators=1, random_state=42)\n",
    "\n",
    "    # 在训练集上训练模型\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    return model\n",
    "\n",
    "def Linear(X_train, X_test, y_train, y_test):\n",
    "    model = LinearRegression()\n",
    "\n",
    "    # 在训练集上训练模型\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    return model\n",
    "\n",
    "def Svr(X_train, X_test, y_train, y_test):\n",
    "    model = SVR()\n",
    "\n",
    "    # 在训练集上训练模型\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    return model\n",
    "\n",
    "def DecisionTree(X_train, X_test, y_train, y_test):\n",
    "    model = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "    # 在训练集上训练模型\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    return model\n",
    "\n",
    "def RidgeRegression(X_train, X_test, y_train, y_test):\n",
    "    model = Ridge(alpha=1.0)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "def GaussianProcess(X_train, X_test, y_train, y_test):\n",
    "    model = GaussianProcessRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "def AdaBoost(X_train, X_test, y_train, y_test):\n",
    "    model = AdaBoostRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "def GradientBoosting(X_train, X_test, y_train, y_test):\n",
    "    model = GradientBoostingRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "def Bagging(X_train, X_test, y_train, y_test):\n",
    "    model = BaggingRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "def ExtraTrees(X_train, X_test, y_train, y_test):\n",
    "    model = ExtraTreesRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "def train_predict_val(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    only for getting the trained models here\n",
    "    \"\"\"\n",
    "    rf = RandomForest(X_train, X_test, y_train, y_test)\n",
    "    lr = Linear(X_train, X_test, y_train, y_test)\n",
    "    svr = Svr(X_train, X_test, y_train, y_test)\n",
    "    dt = DecisionTree(X_train, X_test, y_train, y_test)\n",
    "    rg = RidgeRegression(X_train, X_test, y_train, y_test)\n",
    "    gp = GaussianProcess(X_train, X_test, y_train, y_test)\n",
    "    ada = AdaBoost(X_train, X_test, y_train, y_test)\n",
    "    bag = Bagging(X_train, X_test, y_train, y_test)\n",
    "    ext = ExtraTrees(X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    return rf,lr,svr,dt,rg,gp,ada,bag,ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Tg***\n",
      "Ground truth: 361, 381\n",
      "RF prediction: [274.         330.         394.85       374.54545455 330.\n",
      " 378.875      332.         420.         332.         276.85      ]\n",
      "LR prediction: [[194.85355533]\n",
      " [194.8879272 ]\n",
      " [194.85352993]\n",
      " [194.86679209]\n",
      " [194.88790605]\n",
      " [194.84727622]\n",
      " [194.85125875]\n",
      " [194.92762258]\n",
      " [194.85130049]\n",
      " [194.86684644]]\n",
      "SVR prediction: [228.89999996 228.89999996 228.89999996 228.89999996 228.89999996\n",
      " 228.89999996 228.89999996 228.89999996 228.89999996 228.89999996]\n",
      "DT prediction: [274.   400.   394.85 510.   400.   380.5  321.   330.   321.   276.85]\n",
      "RG prediction: [[289.42955484]\n",
      " [291.77911777]\n",
      " [297.91116513]\n",
      " [313.55233816]\n",
      " [297.32353536]\n",
      " [344.9166446 ]\n",
      " [297.04468341]\n",
      " [317.42285777]\n",
      " [290.60898823]\n",
      " [313.38074568]]\n",
      "GP prediction: [[  0.       ]\n",
      " [  0.       ]\n",
      " [  0.       ]\n",
      " [  0.       ]\n",
      " [  0.       ]\n",
      " [380.4977417]\n",
      " [  0.       ]\n",
      " [  0.       ]\n",
      " [  0.       ]\n",
      " [  0.       ]]\n",
      "ADA prediction: [313.93513816 319.30694444 319.78571739 313.444869   319.78571739\n",
      " 333.17316966 350.76404494 321.0842682  350.76404494 313.444869  ]\n",
      "BAG prediction: [297.685      307.13684211 345.91       326.41978261 307.23684211\n",
      " 380.52459803 313.385      363.87749104 310.255      310.855     ]\n",
      "EXT prediction: [305.1205     311.74883333 316.1825     295.636      316.81683333\n",
      " 380.5        337.493      341.964      336.3815     294.18616667]\n"
     ]
    }
   ],
   "source": [
    "df_val = pd.read_csv('/home/hkqiu/work/PolyGPT/polymer properties/reg-cls/exp_val/data_with_descriptors.csv')\n",
    "df_val = df_val.drop(\"Smiles\", axis=1)\n",
    "\n",
    "print(\"***Tg***\")\n",
    "rf,lr,svr,dt,rg,gp,ada,bag,ext = train_predict_val(tg_X_train, tg_X_test, tg_y_train, tg_y_test)\n",
    "rf_pred = rf.predict(df_val)\n",
    "lr_pred = lr.predict(df_val)\n",
    "svr_pred = svr.predict(df_val)\n",
    "dt_pred = dt.predict(df_val)\n",
    "\n",
    "rg_pred = rg.predict(df_val)\n",
    "gp_pred = gp.predict(df_val)\n",
    "ada_pred = ada.predict(df_val)\n",
    "bag_pred = bag.predict(df_val)\n",
    "\n",
    "ext_pred = ext.predict(df_val)\n",
    "# xgb_pred = xgb.predict(df_val)\n",
    "\n",
    "print(\"Ground truth: 361, 381\")\n",
    "print(f\"RF prediction: {rf_pred}\")\n",
    "print(f\"LR prediction: {lr_pred}\")\n",
    "print(f\"SVR prediction: {svr_pred}\")\n",
    "print(f\"DT prediction: {dt_pred}\")\n",
    "print(f\"RG prediction: {rg_pred}\")\n",
    "print(f\"GP prediction: {gp_pred}\")\n",
    "print(f\"ADA prediction: {ada_pred}\")\n",
    "print(f\"BAG prediction: {bag_pred}\")\n",
    "print(f\"EXT prediction: {ext_pred}\")\n",
    "# print(f\"XGB prediction: {xgb_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 先创建一个空的DataFrame，用于保存预测结果\n",
    "predictions = pd.DataFrame()\n",
    "\n",
    "# 进行预测并将结果保存到DataFrame中\n",
    "predictions['RF'] = rf_pred\n",
    "predictions['LR'] = lr_pred\n",
    "predictions['SVR'] = svr_pred\n",
    "predictions['DT'] = dt_pred\n",
    "predictions['RG'] = rg_pred\n",
    "predictions['GP'] = gp_pred\n",
    "predictions['ADA'] = ada_pred\n",
    "predictions['BAG'] = bag_pred\n",
    "predictions['EXT'] = ext_pred\n",
    "\n",
    "# 将DataFrame保存为CSV文件\n",
    "predictions.to_csv('/home/hkqiu/work/PolyGPT/polymer properties/reg-cls/exp_val/ml_predictions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***atomization energy***\n",
      "RandomForest R2: 0.955150215095907\n",
      "RandomForest MAE: 0.0497435897435901\n",
      "RandomForest MSE: 0.005025641025641048\n",
      "=======================\n",
      "LinearRegression R2: -17.259364423441333\n",
      "LinearRegression MAE: 0.2956599531004891\n",
      "LinearRegression MSE: 2.0460524202024186\n",
      "=======================\n",
      "SVR R2: -0.010928641842724574\n",
      "SVR MAE: 0.27042470423729176\n",
      "SVR MSE: 0.11327957240608147\n",
      "=======================\n",
      "DecisionTree R2: 0.917531313880434\n",
      "DecisionTree MAE: 0.06871794871794865\n",
      "DecisionTree MSE: 0.00924102564102558\n",
      "=======================\n",
      "MLP R2: -86098.6933184913\n",
      "MLP MAE: 29.443596991899526\n",
      "MLP MSE: 9647.898021402405\n",
      "=======================\n"
     ]
    }
   ],
   "source": [
    "print(\"***atomization energy***\")\n",
    "train_predict(ae_X_train, ae_X_test, ae_y_train, ae_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## all properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***all***\n",
      "RandomForest R2: 0.8388710101234426\n",
      "RandomForest MAE: 27.30894414478224\n",
      "RandomForest MSE: 2591.1833496087784\n",
      "=======================\n",
      "LinearRegression R2: 0.002284679254769717\n",
      "LinearRegression MAE: 107.18461334337869\n",
      "LinearRegression MSE: 16044.68152345037\n",
      "=======================\n",
      "SVR R2: -0.3152832949069524\n",
      "SVR MAE: 84.33632152195749\n",
      "SVR MSE: 21151.626261621073\n",
      "=======================\n",
      "DecisionTree R2: 0.827571049549177\n",
      "DecisionTree MAE: 26.761988916256158\n",
      "DecisionTree MSE: 2772.9027888835126\n",
      "=======================\n",
      "Ridge Regression R2: 0.8362969987616238\n",
      "Ridge Regression MAE: 37.012967155790115\n",
      "Ridge Regression MSE: 2632.5771136208186\n",
      "=======================\n",
      "GaussianProcessRegressor R2: -0.2628318323923091\n",
      "GaussianProcessRegressor MAE: 81.3712345257274\n",
      "GaussianProcessRegressor MSE: 20308.13213660548\n",
      "=======================\n",
      "AdaBoostRegressor R2: 0.8540478706203829\n",
      "AdaBoostRegressor MAE: 34.8719277487462\n",
      "AdaBoostRegressor MSE: 2347.1178450143857\n",
      "=======================\n",
      "BaggingRegressor R2: 0.8483749016867923\n",
      "BaggingRegressor MAE: 25.765970519369553\n",
      "BaggingRegressor MSE: 2438.3472547862043\n",
      "=======================\n",
      "ExtraTreesRegressor R2: 0.9029059806371644\n",
      "ExtraTreesRegressor MAE: 22.852215209382265\n",
      "ExtraTreesRegressor MSE: 1561.4099394051702\n",
      "=======================\n"
     ]
    }
   ],
   "source": [
    "X_train = train_df.drop(['prompt','target','task'], axis=1)\n",
    "X_test = test_df.drop(['prompt','target','task'], axis=1)\n",
    "y_train = train_df[['target']]\n",
    "y_test = test_df[['target']]\n",
    "\n",
    "X_train = X_train.head(15654)\n",
    "X_test = X_test.head(1764)\n",
    "y_train = y_train.head(15654)\n",
    "y_test = y_test.head(1764)\n",
    "\n",
    "print(\"***all***\")\n",
    "performance_data = pd.DataFrame(columns=['Model', 'R2', 'MAE', 'MSE'])\n",
    "train_predict(X_train, X_test, y_train, y_test)\n",
    "performance_data.to_csv('./model_performance/reg-all.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "# Create a DataFrame to store the performance data\n",
    "performance_data = pd.DataFrame(columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n",
    "\n",
    "# Function to save performance data\n",
    "def save_performance_data(model_name, accuracy, precision, recall, f1):\n",
    "    global performance_data\n",
    "    new_row = pd.DataFrame([[model_name, accuracy, precision, recall, f1]], columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n",
    "    performance_data = pd.concat([performance_data, new_row], ignore_index=True)\n",
    "\n",
    "# Function to train and evaluate the Random Forest model\n",
    "def RandomForest(X_train, X_test, y_train, y_test):\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    model = RandomForestClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    save_performance_data('Random Forest', accuracy, precision, recall, f1)\n",
    "    print(f\"Random Forest Accuracy: {accuracy}\")\n",
    "    print(f\"Random Forest Precision: {precision}\")\n",
    "    print(f\"Random Forest Recall: {recall}\")\n",
    "    print(f\"Random Forest F1 Score: {f1}\")\n",
    "    print(\"=======================\")\n",
    "\n",
    "# Function to train and evaluate the Logistic Regression model\n",
    "def LogisticReg(X_train, X_test, y_train, y_test):\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    save_performance_data('Logistic Regression', accuracy, precision, recall, f1)\n",
    "    print(f\"Logistic Regression Accuracy: {accuracy}\")\n",
    "    print(f\"Logistic Regression Precision: {precision}\")\n",
    "    print(f\"Logistic Regression Recall: {recall}\")\n",
    "    print(f\"Logistic Regression F1 Score: {f1}\")\n",
    "    print(\"=======================\")\n",
    "\n",
    "# Function to train and evaluate the SVC model\n",
    "def SVC_Classifier(X_train, X_test, y_train, y_test):\n",
    "    from sklearn.svm import SVC\n",
    "    model = SVC()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    save_performance_data('SVC', accuracy, precision, recall, f1)\n",
    "    print(f\"SVC Accuracy: {accuracy}\")\n",
    "    print(f\"SVC Precision: {precision}\")\n",
    "    print(f\"SVC Recall: {recall}\")\n",
    "    print(f\"SVC F1 Score: {f1}\")\n",
    "    print(\"=======================\")\n",
    "\n",
    "# Function to train and evaluate the Decision Tree model\n",
    "def DecisionTree(X_train, X_test, y_train, y_test):\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    model = DecisionTreeClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    save_performance_data('Decision Tree', accuracy, precision, recall, f1)\n",
    "    print(f\"Decision Tree Accuracy: {accuracy}\")\n",
    "    print(f\"Decision Tree Precision: {precision}\")\n",
    "    print(f\"Decision Tree Recall: {recall}\")\n",
    "    print(f\"Decision Tree F1 Score: {f1}\")\n",
    "    print(\"=======================\")\n",
    "\n",
    "# Function to train and evaluate the AdaBoost model\n",
    "def AdaBoost(X_train, X_test, y_train, y_test):\n",
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "    model = AdaBoostClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    save_performance_data('AdaBoost', accuracy, precision, recall, f1)\n",
    "    print(f\"AdaBoost Accuracy: {accuracy}\")\n",
    "    print(f\"AdaBoost Precision: {precision}\")\n",
    "    print(f\"AdaBoost Recall: {recall}\")\n",
    "    print(f\"AdaBoost F1 Score: {f1}\")\n",
    "    print(\"=======================\")\n",
    "\n",
    "# Function to train and evaluate the XGBoost model\n",
    "def XGBoost(X_train, X_test, y_train, y_test):\n",
    "    # 将字符串类别标签转换为数值\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "    y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "    # 训练 XGBoost 模型\n",
    "    model = xgb.XGBClassifier()\n",
    "    model.fit(X_train, y_train_encoded)\n",
    "    y_pred_encoded = model.predict(X_test)\n",
    "\n",
    "    # 将预测标签转换回原始的字符串类别标签\n",
    "    y_pred = label_encoder.inverse_transform(y_pred_encoded)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    save_performance_data('XGBoost', accuracy, precision, recall, f1)\n",
    "    print(f\"XGBoost 精确度：{precision}\")\n",
    "    print(f\"XGBoost 召回率：{recall}\")\n",
    "    print(f\"XGBoost F1 分数：{f1}\")\n",
    "    print(\"=======================\")\n",
    "\n",
    "# Function to train and evaluate the K-Nearest Neighbors model\n",
    "def KNN(X_train, X_test, y_train, y_test):\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    model = KNeighborsClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    save_performance_data('K-Nearest Neighbors', accuracy, precision, recall, f1)\n",
    "    print(f\"K-Nearest Neighbors Accuracy: {accuracy}\")\n",
    "    print(f\"K-Nearest Neighbors Precision: {precision}\")\n",
    "    print(f\"K-Nearest Neighbors Recall: {recall}\")\n",
    "    print(f\"K-Nearest Neighbors F1 Score: {f1}\")\n",
    "    print(\"=======================\")\n",
    "\n",
    "# Function to train and evaluate the Naive Bayes model\n",
    "def NaiveBayes(X_train, X_test, y_train, y_test):\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    model = GaussianNB()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    save_performance_data('Naive Bayes', accuracy, precision, recall, f1)\n",
    "    print(f\"Naive Bayes Accuracy: {accuracy}\")\n",
    "    print(f\"Naive Bayes Precision: {precision}\")\n",
    "    print(f\"Naive Bayes Recall: {recall}\")\n",
    "    print(f\"Naive Bayes F1 Score: {f1}\")\n",
    "    print(\"=======================\")\n",
    "\n",
    "def cls_train_predict(X_train, X_test, y_train, y_test):\n",
    "    RandomForest(X_train, X_test, y_train, y_test)\n",
    "    LogisticReg(X_train, X_test, y_train, y_test)\n",
    "    SVC_Classifier(X_train, X_test, y_train, y_test)\n",
    "    DecisionTree(X_train, X_test, y_train, y_test)\n",
    "    AdaBoost(X_train, X_test, y_train, y_test)\n",
    "    KNN(X_train, X_test, y_train, y_test)\n",
    "    NaiveBayes(X_train, X_test, y_train, y_test)\n",
    "    XGBoost(X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.7297297297297297\n",
      "Random Forest Precision: 0.6655405405405406\n",
      "Random Forest Recall: 0.7297297297297297\n",
      "Random Forest F1 Score: 0.6954954954954955\n",
      "=======================\n",
      "Logistic Regression Accuracy: 0.08108108108108109\n",
      "Logistic Regression Precision: 0.006574141709276844\n",
      "Logistic Regression Recall: 0.08108108108108109\n",
      "Logistic Regression F1 Score: 0.012162162162162163\n",
      "=======================\n",
      "SVC Accuracy: 0.5675675675675675\n",
      "SVC Precision: 0.32213294375456536\n",
      "SVC Recall: 0.5675675675675675\n",
      "SVC F1 Score: 0.4109972041006524\n",
      "=======================\n",
      "Decision Tree Accuracy: 0.6756756756756757\n",
      "Decision Tree Precision: 0.6400635930047694\n",
      "Decision Tree Recall: 0.6756756756756757\n",
      "Decision Tree F1 Score: 0.6537903757415953\n",
      "=======================\n",
      "AdaBoost Accuracy: 0.4864864864864865\n",
      "AdaBoost Precision: 0.7913851351351351\n",
      "AdaBoost Recall: 0.4864864864864865\n",
      "AdaBoost F1 Score: 0.4251651651651651\n",
      "=======================\n",
      "K-Nearest Neighbors Accuracy: 0.7027027027027027\n",
      "K-Nearest Neighbors Precision: 0.7327702702702703\n",
      "K-Nearest Neighbors Recall: 0.7027027027027027\n",
      "K-Nearest Neighbors F1 Score: 0.7016002545859569\n",
      "=======================\n",
      "Naive Bayes Accuracy: 0.35135135135135137\n",
      "Naive Bayes Precision: 0.12344777209642074\n",
      "Naive Bayes Recall: 0.35135135135135137\n",
      "Naive Bayes F1 Score: 0.18270270270270272\n",
      "=======================\n",
      "XGBoost 精确度：0.6655405405405406\n",
      "XGBoost 召回率：0.7297297297297297\n",
      "XGBoost F1 分数：0.6954954954954955\n",
      "=======================\n"
     ]
    }
   ],
   "source": [
    "cls_train_predict(hrc_X_train, hrc_X_test, hrc_y_train, hrc_y_test)\n",
    "performance_data.to_csv('./model_performance/heat resistence.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
